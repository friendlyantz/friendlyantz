---
layout: single
# title: "Vi Everywhere"
permalink: /proxmox/
excerpt: "proxmox notes"
# last_modified_at: 2016-11-03T11:13:12-04:00
collection: learning
categories:
  - software_engineering
  - learning
tags:
  - software_engineering
  - learning
  - proxmox
  - linux
  - vm
  - containers
  - pfsense
  - networking
  - firewall
  - photoprism
  - self-hosted
  - home-server
  - home-lab
  - home-automation
  - home-network
---

[Superb Tutorial, however I put some additional notes below](https://www.youtube.com/watch?v=LCjuiIswXGs&list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo)

> but I would diverge from some of the defaults they demo as they don't work or there are better and faster alternatives(notes below)

* Table of contents (do not remove this line)
{:toc}

## VM vs Containers

* You can migrate VMs between clusters w/o shutting them down;
* Containers are more suitable if you have limited resources and are very efficient on RAM;
* Not all apps will run properly in containers;
* Some vendors have stigma against containers;

# Linux VM

Highly recommend running [Ubuntu Server](https://ubuntu.com/download/server), as it has convenient way to enable SSH keys without password from the install and scrape those keys from the GitHub, you don't have to install essential packes like `sude`, `curl`, `git` like on Debian, and it has a nicer way to handle missing packages.
I used originally [Debian](https://www.debian.org/download), Parrot OS Arch distro failed to install.
Ubuntu server failed originally because i had BIOS boot mod enabled(which is default, make sure to chage it to UEFI when creating VM)

## 'Create New VM' Options

* [ ] Disk: change Bus from SCSI to VirtIO Block (fastest) and enable 'Discard'
* [ ] Disk: change boot from BIOS to UEFI (Ubuntu Server wont work with BIOS, Debian will though)
* [ ] CPU: type -> `host`
* [ ] Netwrok: Model -> `Intel E1000` if it works for you, otherwise default

## Post Install

- [ ] Unmount boot CD --> VM Hardware

## Install SSH

[my_ssh_article]({% link _learning/2023-04-18-ssh.md %})
- [ ] ensure to disable password login in `/etc/ssh/sshd_config`

## enable root access for users if you want

```sh
sudo usermod -aG sudo friendlyantz
```

### Enable QEMU Guest Agent 
This will help with sending proper power off / exit commands from Proxmox to the VM:

* [ ] Install QEMU Guest Agent

```sh
sudo apt install qemu-guest-agent
systemctl status qemu-guest-agent.service # check if running
# -> enable in VM Options
systemctl start qemu-guest-agent.service # if not running
```

* [ ] and enable it in Proxmox VM Options(restart required)

### Install ZeroTier
```sh
curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \
if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo "$z" | sudo bash; fi
```

# Generic tips
- [ ] disable 'use tablet for pointer' if you don't use OS GUI --> VM Options
- [ ] Separate Proxmox management network from VM networks
- [ ] Enable start VM at boot if required --> Options
- [ ] VM a CD / iso to transfer files between machines (WIP)

# Photoprism

Photoprism is a self-hosted photo management application available for Linux, Windows, Mac and NAS devices. It is a Go application, available as a single executable file. It is cross-platform and can be easily deployed using Docker.

Ensure you change password for 1. Client and MarianDB from _insecure_ default password
Also when mounting volumes be explicit and state full path i.e. `/home/friendlyantz/Pictures/photoprism_originals` instead of `~/Pictures/photoprism_originals` as the latter can put `Pictures` forlder under `/root` instead of `/home/friendlyantz`
```yml
      - "/home/friendlyantz/Pictures/photoprism_originals:/photoprism/originals"               # Original media files (DO NOT REMOVE)
      - "/home/friendlyantz/Pictures/Archive:/photoprism/originals/archive" # *Additional* media folders can be mounted like this
```
Also, 1st line is for originals, 2nd for additional folders. Left side is your local path, right side is the path inside the container and will be displayed in UI are `/archive` in this case.


# Networking / Firewall (WIP)

## Options and Considerations

1. Internet Router --> Managable Switch --> 
  - Server with PFSense AND  
  - Access Points for 
    - separate VLANs / PFSense tags --> Devices
    - Secure Net
    - Home Automation 
    - etc
2. Internet Router w/o switch
  - LAN to Proxmox with PFSense 
    - --> USB WiFi Dongle to use as Access Point for IoT devices
    - Normal internet / WiFi for Secure Net
3. Internet Router with VLANs
  - LAN to Proxmox with PFSense to tag VLANs
    - VLAN for IoT
    - VLAN for Secure Net
    - VLAN for Home Automation / etc

## PFsense in Proxmox

1. Proxmox ---> Ensure you create a virtual network bridge that you can use as LAN for PFSense, because as soon as you create another WLAN / network firewall will block all you from accessing original ETH/WAN ip address
1. pfSense/Interfaces: Create new LAN assigngment for your LAN bridge
1. pfSense/Interfaces: Create new OPT assigngment for your WLAN
1. DHCP Server: Enable DHCP server for WLAN

## Add USB dongle

> WIP

# External Storage

```sh
fdisk -l # to find the drive

mkdir /mnt/usb_data

mount /dev/sdb1 /mnt/usb_data # mount the drive
```

# Create Templates

## Sanitize

### SSH

[tutorial](https://www.youtube.com/watch?v=E8VjZ62Ns6Y)

```sh
ls -l /etc/ssh 
```

* [ ] Purge SSH keys from /etc/ssh

```sh
sudo rm /etc/ssh/ssh_host_* # remove host keys
```

* [ ] ensure `cloud-init` package is already present - this SUPPOSED to generate ssh keys on the install, but it doesn't work for me, below

```sh
apt search cloud-init # check if it already installed
# or
sudo apt install cloud-init # install if not
```

or do it manually 

1. Under `/etc/systemd/system/` dir create a file as per this [script](https://gist.github.com/friendlyantz/7c69c7e2749ce9f2f1f3cefd998f180a)
2. change ownership to root

```sh
sudo chown root:root regenerate_ssh_host_keys.service
```

3. reavaluate systemd
```sh
sudo systemctl daemon-reload
```
4. check if it's enabled
```sh
systemctl status regenerate_ssh_host_keys.service
# AND enable it
sudo systemctl enable regenerate_ssh_host_keys.service
```

### Reset Machine ID

```sh
cat /etc/machine-id # (on Ubuntu) check if it's empty. do not remove
# empty it if it's not

truncate -s 0 /etc/machine-id 
```

Check symbolic link is pointing to `/etc/machine-id`

```sh
ls -l /var/lib/dbus/machine-id
# otherwise, link it
sudo ln -s /etc/machine-id /var/lib/dbus/machine-id
```

### Clean `apt` database and package cache

```sh
sudo apt clean
sudo apt autoremove
```

### Convert VM to template (via Proxmox UI)

- [ ] Now power-off the VM and convert it to template

#### Replace CD with Cloud-init Drive

- [ ] Add Cloud-init Drive --> Hardware tab

#### setup Cloud-init

- [ ] Set up default user in cloud init UI tab

#### add SSH public keys to cloud-init

### Create new VM from template

* [ ] select mode Full vs Linked Clone(VM will die if template gets removed)
* [ ] upd hostname `vi /etc//hosts`

# User Management
