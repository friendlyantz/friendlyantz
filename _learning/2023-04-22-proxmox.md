---
layout: single
permalink: /proxmox/
excerpt: proxmox notes
collection: learning
categories:
  - software_engineering
  - learning
tags:
  - proxmox
  - linux
  - vm
  - containers
  - pfsense
  - networking
  - firewall
  - photoprism
  - self-hosted
  - firewall
  - security
  - ssh
---

[Superb Tutorial, however I put some additional notes below](https://www.youtube.com/watch?v=LCjuiIswXGs&list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo)

> but I would diverge from some of the defaults they demo as they don't work or there are better and faster alternatives(notes below)

## VM vs Containers

* You can migrate VMs between clusters w/o shutting them down;
* Containers are more suitable if you have limited resources and are very efficient on RAM;
* Not all apps will run properly in containers;
* Some vendors have stigma against containers;

## Linux VM

Highly recommend running [Ubuntu Server](https://ubuntu.com/download/server), as it has convenient way to enable SSH keys without password from the install and scrape those keys from the GitHub, you don't have to install essential packes like `sude`, `curl`, `git` like on Debian, and it has a nicer way to handle missing packages.
I used originally [Debian](https://www.debian.org/download), Parrot OS Arch distro failed to install.
Ubuntu server failed originally because i had BIOS boot mod enabled(which is default, make sure to chage it to UEFI when creating VM)

## 'Create New VM' Options

* [ ] Disk: change Bus from SCSI to VirtIO Block (fastest) and enable 'Discard'
* [ ] Disk: change boot from BIOS to UEFI (Ubuntu Server wont work with BIOS, Debian will though)
* [ ] CPU: type -> `host`
* [ ] Netwrok: Model -> `Intel E1000` if it works for you, otherwise default

## Post Install

- [ ] Unmount boot CD --> VM Hardware
- [ ] Run post-install script to clean-up pop-up notification

### Install SSH

[my_ssh_article]({% link _learning/2023-04-18-ssh.md %})
- [ ] ensure to disable password login in `/etc/ssh/sshd_config`

### enable root access for users if you want

```sh
sudo usermod -aG sudo friendlyantz
```

### Enable QEMU Guest Agent 
This will help with sending proper power off / exit commands from Proxmox to the VM:

* [ ] Install QEMU Guest Agent

```sh
sudo apt install qemu-guest-agent
systemctl status qemu-guest-agent.service # check if running
# -> enable in VM Options
systemctl start qemu-guest-agent.service # if not running
```

* [ ] and enable it in Proxmox VM Options(restart required)

### Install ZeroTier
```sh
curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \
if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo "$z" | sudo bash; fi
```

## Generic tips
- [ ] disable 'use tablet for pointer' if you don't use OS GUI --> VM Options
- [ ] Separate Proxmox management network from VM networks
- [ ] Enable start VM at boot if required --> Options
- [ ] VM a CD / iso to transfer files between machines (WIP)

## Photoprism

Photoprism is a self-hosted photo management application available for Linux, Windows, Mac and NAS devices. It is a Go application, available as a single executable file. It is cross-platform and can be easily deployed using Docker.

Ensure you change password for 1. Client and MarianDB from _insecure_ default password
Also when mounting volumes be explicit and state full path i.e. `/home/friendlyantz/Pictures/photoprism_originals` instead of `~/Pictures/photoprism_originals` as the latter can put `Pictures` forlder under `/root` instead of `/home/friendlyantz`
```yml
      - "/home/friendlyantz/Pictures/photoprism_originals:/photoprism/originals"               # Original media files (DO NOT REMOVE)
      - "/home/friendlyantz/Pictures/Archive:/photoprism/originals/archive" # *Additional* media folders can be mounted like this
```
Also, 1st line is for originals, 2nd for additional folders. Left side is your local path, right side is the path inside the container and will be displayed in UI are `/archive` in this case.


## Networking / Firewall (WIP)

### Options and Considerations

1. Internet Router --> Managable Switch --> 
  - Server with PFSense AND  
  - Access Points for 
    - separate VLANs / PFSense tags --> Devices
    - Secure Net
    - Home Automation 
    - etc
2. Internet Router w/o switch
  - LAN to Proxmox with PFSense 
    - --> USB WiFi Dongle to use as Access Point for IoT devices
    - Normal internet / WiFi for Secure Net
3. Internet Router with VLANs
  - LAN to Proxmox with PFSense to tag VLANs
    - VLAN for IoT
    - VLAN for Secure Net
    - VLAN for Home Automation / etc

### PFsense in Proxmox

1. Proxmox ---> Ensure you create a virtual network bridge that you can use as LAN for PFSense, because as soon as you create another WLAN / network firewall will block all you from accessing original ETH/WAN ip address
1. pfSense/Interfaces: Create new LAN assigngment for your LAN bridge
1. pfSense/Interfaces: Create new OPT assigngment for your WLAN
1. DHCP Server: Enable DHCP server for WLAN

### Add USB dongle

> WIP

## External Storage

```sh
fdisk -l # to find the drive

mkdir /mnt/usb_data

mount /dev/sdb1 /mnt/usb_data # mount the drive
```

```sh
lsblk # to find the drive
mkfs -t ext4 /dev/sda # format the drive OR do it via UI
# encryption
cryptsetup luksFormat /dev/sda # encrypt the drive
```

## Create VM Templates

### Sanitize

#### SSH

[tutorial](https://www.youtube.com/watch?v=E8VjZ62Ns6Y)

```sh
ls -l /etc/ssh 
```

* [ ] Purge SSH keys from /etc/ssh

```sh
sudo rm /etc/ssh/ssh_host_* # remove host keys
```

* [ ] ensure `cloud-init` package is already present - this SUPPOSED to generate ssh keys on the install, but it doesn't work for me, below

```sh
apt search cloud-init # check if it already installed
# or
sudo apt install cloud-init # install if not
```

or do it manually 

1. Under `/etc/systemd/system/` dir create a file as per this [script](https://gist.github.com/friendlyantz/7c69c7e2749ce9f2f1f3cefd998f180a)
2. change ownership to root

```sh
sudo chown root:root regenerate_ssh_host_keys.service
```

3. reavaluate systemd
```sh
sudo systemctl daemon-reload
```
4. check if it's enabled
```sh
systemctl status regenerate_ssh_host_keys.service
# AND enable it
sudo systemctl enable regenerate_ssh_host_keys.service
```

#### Reset Machine ID

```sh
cat /etc/machine-id # (on Ubuntu) check if it's empty. do not remove
# empty it if it's not

truncate -s 0 /etc/machine-id 
```

Check symbolic link is pointing to `/etc/machine-id`

```sh
ls -l /var/lib/dbus/machine-id
# otherwise, link it
sudo ln -s /etc/machine-id /var/lib/dbus/machine-id
```

#### Clean `apt` database and package cache

```sh
sudo apt clean
sudo apt autoremove
```

### Convert VM to template (via Proxmox UI)

- [ ] Now power-off the VM and convert it to template

#### Replace CD with Cloud-init Drive

- [ ] Add Cloud-init Drive --> Hardware tab

#### setup Cloud-init

- [ ] Set up default user in cloud init UI tab

- [ ] add SSH authorized public keys to cloud-init

### Setup Firewall rules and enable it via Proxmox UI

### Create new VM from template

- [ ] remove password auth - Cloud-init allows user to login with password by default, so disable it
```sh
sudo vi /etc/ssh/sshd_config.d/
sudo vi /etc/ssh/sshd_config
```

---

### Create new VM from template

* [ ] select mode Full vs Linked Clone(VM will die if template gets removed)
* [ ] upd hostname `vi /etc//hosts`

## Create Container Templates

```sh
sudo apt clean # clean app cache
sudo apt autoremove # remove unused packages

# purge SSH host keys
cd /etc/ssh/ # remove host keys
sudo rm ssh_host_* # remove host keys (do not leave ssh session after this)

# reset machine ID
cat /etc/machine-id # (on Ubuntu) check if it's empty. do not remove
sudo truncate -s 0 /etc/machine-id # empty it if it's not

sudo poweroff # power off the VM
```

- [ ] Convert Container to template (via Proxmox UI)

## User Management

In DataCenter tab:

1. Create Proxmox VE(PVE) realm users, not Linux PAM users(unless you need SSH and SHell access to Proxmox server)
1. Create group
1. Add permissions to the group
1. Add group to the user

## Backups and Snapshots

### Snapshots

Are good for testing software, but not for backups. They are stored in the same storage as the VM, so if the storage dies, so does the snapshot.

### Backups

Use datacenter to manage and schedule Backups across all VMs, not just individual VM.

Consider between different modes:

* snapshot(not 100% reliable, but still pretty reliable and no downtime)
* stop(reliable, but has downtime)
* suspend(reliable, but has downtime)

## Firewall

Remove all access unless required.
Top-down rule: start at Datacenter --> Cluster / Host --> VM
Rules are applied just to that level(i.e Datacenter, Cluster, VM), they are not inherited.

## Add new rule (some examples below)

| setting / protocol | tcp(for web ui / Proxmox console[note this also enables SSH if you do not specify the port]) | icmp(for ping, etc, OPTIONAL) | tcp for SSH |
| --- | --- | --- |                                            --- |
| Macro | --- | --- |                                            SSH |
| Protocol | tcp | icmp |                                      tcp(not required if using Macro) |
| Interface| `vmbr0` | --- |                                   --- |
| Source port | 8006 | --- |                                    --- |
| Destination port | 8006 | --- |                              22(not required if using Macro) |
| Source | --- | `ip_of_your_machine`/32  |                    --- |

tick enabled

Note: TCP without specified port also enables SSH, so you can use that instead of Macro

### Firewall Options

- [ ] Enable: `YES`
- [ ] Input / Output policies

## Proxmox CLI

### For VMs

```sh
qm list # list all VMs
qm start 100 # start VM with ID 100

qm shutdown 100 # graceful shutdown VM with ID 100
qm reboot 100 # graceful reboot VM with ID 100

qm reset 100 # hard reset VM with ID 100
qm stop 100 # hard stop VM with ID 100

qm set --onboot 0 106 # disable autostart for VM with ID 106
qm config 106 # show config for VM with ID 106
# RTFM
man qm
```

# Containers
