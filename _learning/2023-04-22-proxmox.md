---
layout: single
permalink: /proxmox/
excerpt: proxmox notes
collection: learning
categories:
  - software_engineering
  - learning
tags:
  - proxmox
  - linux
  - vm
  - containers
  - pfsense
  - networking
  - firewall
  - photoprism
  - self-hosted
  - firewall
  - security
  - ssh
---

[Superb Tutorial, however I put some additional notes below](https://www.youtube.com/watch?v=LCjuiIswXGs&list=PLT98CRl2KxKHnlbYhtABg6cF50bYa8Ulo)

> but I would diverge from some of the defaults they demo as they don't work or there are better and faster alternatives(notes below)

## INTRO: VM vs Containers

* TLDR: use VMs
* You can migrate VMs between clusters w/o shutting them down;
* Containers are more suitable if you have limited resources and are very efficient on RAM;
* Not all apps will run properly in containers;
* Some vendors have stigma against containers;
* Containers are a pain to setUp with zeroTier if they are unprivileged, SSH will be a pain after switching to priviliged
* Proxmox CLI `qm` commands work only on VMs

## Linux VM

Highly recommend running [Ubuntu Server](https://ubuntu.com/download/server), as it has convenient way to enable SSH keys without password from the install and scrape those keys from the GitHub, you don't have to install essential packes like `sude`, `curl`, `git` like on Debian, and it has a nicer way to handle missing packages.
I trie originally [Debian](https://www.debian.org/download), Parrot OS Arch distro failed to install.
> Ubuntu server failed originally because i had BIOS boot mod enabled(which is default, make sure to chage it to UEFI when creating VM)

### 'Create New VM' Options

* [ ] System: BIOS: UEFI for ubuntu (as per above)
* [ ] ~~ Disk: change boot from BIOS to UEFI (Ubuntu Server wont work with BIOS, Debian will though) ~~
* [ ] Disk: change Bus from SCSI to VirtIO Block (fastest) and enable 'Discard'
* [ ] 
* [ ] CPU: type -> `host`
* [ ] Netwrok: Model -> `Intel E1000` if it works for you, otherwise default

---
### Post Install - VMs

- [ ] wait for Cloudnit to configure SSH keys - you will see it in terminal
- [ ] Unmount boot CD --> VM Hardware
- [ ] basic system update
```sh
cat /proc/cpuinfo

sudo apt update && sudo apt dist-upgrade
```
- [ ] install QEMU (see section below)
- [ ] Run post-install script to clean-up pop-up notification
- [ ] if `ubuntu` consider ubuntu PRO by running: `pro attach`
- [ ] disable 'use tablet for pointer' if you don't use OS GUI --> VM Options
- [ ] Separate Proxmox management network from VM networks
- [ ] Enable start VM at boot if required --> Options
- [ ] Provision a CD / iso to transfer files between machines

----
#### Install SSH

[my_ssh_article]({% link _learning/2023-04-18-ssh.md %})
- [ ] ensure to disable password login in `/etc/ssh/sshd_config`
```sh
vi /etc/ssh/sshd_config
systemctl restart sshd
```
- [ ] reboot schedule
```
0 4 * * * root /usr/sbin/reboot
```
check status
```sh
sudo systemctl status ssh
```
#### enable root access for users if you want

```sh
sudo usermod -aG sudo friendlyantz
```

#### Enable QEMU Guest Agent 
This will help with sending proper power off / exit commands from Proxmox to the VM:

* [ ] Install QEMU Guest Agent

```sh
sudo apt install qemu-guest-agent
systemctl status qemu-guest-agent.service # check if running
# -> enable in VM Options
systemctl start qemu-guest-agent.service # if not running
```

* [ ] and enable it in Proxmox VM Options(restart required)

#### Docker setup (instructions from Ruby on Rails Kamal)

```
sudo apt update
sudo apt upgrade -y
sudo apt install -y docker.io curl git
sudo usermod -a -G docker friendlyantz
```
#### Install ZeroTier(unless template)
```sh
# less secure
curl -s https://install.zerotier.com | sudo bash

# more secure
curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40www.zerotier.com.gpg' | gpg --import && \  
if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo "$z" | sudo bash; fi

# join your network
sudo zerotier-cli join 12345

# ensure you are connected
zerotier-cli listnetworks
```

#### Set static IP for local network instead of DHCP

```sh
sudo vi /etc/netplan/00-installer-config.yaml

# This is the network config written by 'subiquity'
network:
  ethernets:
    ens18:
      dhcp4: no
      addresses: [192.168.1.10/24]
      gateway4: 192.168.1.1
  version: 2
```

---

## Containers ðŸ«™

LXC containers - save state
Installed from ubuntu / etc server template

- Network: DHCP, or if you want static IP
	- IPv4/CIDR: `192.168.1.xx/24`
	- Gateway: `192.168.1.1`
	- 
### Post install - containers

- [ ] start automatically
#### check ssh
```sh
sudo systemctl status ssh
```

you might need new key if cloned from template
```sh
sudo ssh-keygen -A

sudo systemctl restart ssh # i did sshd
```
- [ ] unprivileged contained - safer (no root account mapping on the host system), but might cause issues
```sh
ip a # check ip
```
- [ ] add user
```sh
adduser friendlyantz
### fill details
usermod -aG sudo friendlyantz
```
 refer VM post install above(make sure to add public ssh key before disabling password)
 - !!! **disabling password loging might cause issues for template** - you might need to restart SSHD (server via Proxmox UI CLI)
 
#### ZeroTier / tailscale
 
 [forum link to resolve ZT not connecting](https://forum.level1techs.com/t/zerotier-in-lxc-proxmox/155515/11)
- TLDR:
> Essentially you need to give the LXC container the permissions to be able to create TUN devices. My LXC containers are on Proxmox, so my instructions are based on that.

 go to config. Change `unprivilaged setting` - this will break SSH
```sh
vi /etc/pve/lxc/XXX.conf # replace XXX with your container ID

# change unpriviliged to 0 (read above about dangers)
unpriviliged: 0
```

for older Proxmox
```sh
lxc.cgroup.devices.allow = c 10:200 rwm
lxc.hook.autodev = sh -c "modprobe tun; cd ${LXC_ROOTFS_MOUNT}/dev; mkdir net; mknod net/tun c 10 200; chmod 0666 net/tun"
```
or for Proxmox 7
```sh
lxc.cgroup2.devices.allow = c 10:200 rwm
lxc.hook.autodev = sh -c "modprobe tun; cd ${LXC_ROOTFS_MOUNT}/dev; mkdir net; mknod net/tun c 10 200; chmod 0666 net/tun"
```



---

## Create VM Templates

### Sanitize

#### SSH

[tutorial](https://www.youtube.com/watch?v=E8VjZ62Ns6Y)

```sh
ls -l /etc/ssh 
```

* [ ] Purge SSH keys from /etc/ssh

```sh
sudo rm /etc/ssh/ssh_host_* # remove host keys
```

* [ ] ensure `cloud-init` package is already present - this SUPPOSED to generate ssh keys on the install, but it doesn't work for me, below

```sh
apt search cloud-init # check if it already installed
# or
sudo apt install cloud-init # install if not
```

or do it manually 

1. Under `/etc/systemd/system/` dir create a file as per this [script](https://gist.github.com/friendlyantz/7c69c7e2749ce9f2f1f3cefd998f180a)
2. change ownership to root

```sh
sudo chown root:root regenerate_ssh_host_keys.service
```

3. reavaluate systemd
```sh
sudo systemctl daemon-reload
```
4. check if it's enabled
```sh
systemctl status regenerate_ssh_host_keys.service
# AND enable it
sudo systemctl enable regenerate_ssh_host_keys.service
```

#### Reset Machine ID

```sh
# (on Ubuntu) check if it's empty. do not remove
cat /etc/machine-id

# empty it if it's not
truncate -s 0 /etc/machine-id 
```

Check symbolic link is pointing to `/etc/machine-id`

```sh
ls -l /var/lib/dbus/machine-id
# otherwise, link it
sudo ln -s /etc/machine-id /var/lib/dbus/machine-id
```

#### Clean `apt` database and package cache

```sh
sudo apt clean
sudo apt autoremove
```

### Convert VM to template (via Proxmox UI)

- [ ] Now power-off the VM and convert it to template

#### Replace CD with Cloud-init Drive

- [ ] Add Cloud-init Drive --> Hardware tab (keep IDE)

#### setup Cloud-init

- [ ] Set up default user in cloud init UI tab
- [ ] add SSH authorized public keys to cloud-init
- [ ] Click `Regenerate Image`

### Setup Firewall rules and enable it via Proxmox UI

## ðŸŒ±new VM from template

* [ ] select mode Full vs Linked Clone(VM will die if template gets removed)
- [ ] Ubuntu - UEFI
- [ ] DIsk -> local lvm?
- [ ] check SSH status
```sh
sudo systemctl status ssh

# if CloudInit stuffed up hostkey generation:
sudo ssh-keygen -A
sudo systemctl start ssh
```
- [ ] remove password auth - Cloud-init allows user to login with password by default, so disable it
```sh
sudo vi /etc/ssh/sshd_config.d/
sudo vi /etc/ssh/sshd_config
```

* [ ] upd hostname `vi /etc/hosts`

## Create Container Templates

```sh
sudo apt update && sudo apt dist-upgrade # final update before clean
#  ight install these packages
sudo apt install curl net-tools fzf


sudo apt clean # clean app cache
sudo apt autoremove # remove unused packages

# purge SSH host keys (ONLY IF YOU KEEP PASSWORD LOGIN FOR TEMPLATE, otherwise you might need to restart SSHD server via Proxmox CLI)
cd /etc/ssh/ # remove host keys
sudo rm ssh_host_* # remove host keys (do not leave ssh session after this)

# reset machine ID
cat /etc/machine-id # (on Ubuntu) check if it's empty. do not remove
sudo truncate -s 0 /etc/machine-id # empty it if it's not

sudo poweroff # power off the VM
```

- [ ] Convert Container to template (via Proxmox UI) - ***Full Clone***
	- [ ] pick purpose-built hostname

- after login
```
sudo apt update && sudo apt dist-upgrade
```

- [ ] there is no cloudInit for Containers so we need to setup ssh 

```sh
cd /etc/ssh/ # remove host keys
sudo rm ssh_host_* # remove host keys (do not leave ssh session after this)
sudo dpkg-reconfigure openssh-server # reconfigure ssh server host keys
```
## Proxmox Admin / User Management

In DataCenter tab:

1. Create Proxmox VE(PVE) realm users, not Linux PAM users(unless you need SSH and SHell access to Proxmox server)
1. Create group
1. Add permissions to the group
1. Add group to the user

---
## Backups and Snapshots

### Snapshots

Are good for testing software, but not for backups. They are stored in the same storage as the VM, so if the storage dies, so does the snapshot.

### Backups

Use datacenter to manage and schedule Backups across all VMs, not just individual VM.

Consider between different modes:

* snapshot(not 100% reliable, but still pretty reliable and no downtime)
* stop(reliable, but has downtime)
* suspend(reliable, but has downtime)


---
## Proxmox CLI

For VM only (not CONTAINERS!)

```sh
qm list # list all VMs
qm start 100 # start VM with ID 100

qm shutdown 100 # graceful shutdown VM with ID 100
qm reboot 100 # graceful reboot VM with ID 100

qm reset 100 # hard reset VM with ID 100
qm stop 100 # hard stop VM with ID 100

qm set --onboot 0 106 # disable autostart for VM with ID 106
qm config 106 # show config for VM with ID 106
# RTFM
man qm
```

---



## Photoprism

options:
### opt1 - on top of LXC container install (ZeroTier will be a pain)
### opt2 - VM (preferred)
---
3. install [docker](https://docs.photoprism.app/getting-started/troubleshooting/docker/#installation), easiest way below:
```sh
bash <(curl -s https://setup.photoprism.app/ubuntu/install-docker.sh)
```
### config

- [ ] Ensure you change password for 1. Client and MarianDB from _insecure_ default password
- [ ] Also when mounting volumes be explicit and state full path i.e. `/home/friendlyantz/Pictures/photoprism_originals` instead of `~/Pictures/photoprism_originals` as the latter can put `Pictures` forlder under `/root` instead of `/home/friendlyantz`
```yml
      - "/home/friendlyantz/Pictures/photoprism_originals:/photoprism/originals"               # Original media files (DO NOT REMOVE)
      - "/home/friendlyantz/Pictures/Archive:/photoprism/originals/archive" # *Additional* media folders can be mounted like this
```
> Also, 1st line is for originals, 2nd for additional folders. Left side is your local path, right side is the path inside the container and will be displayed in UI are `/archive` in this case.

### docker compose up

```sh
sudo docker compose up -d
```

#### set up auto boot docker compose up in case of reboot/powerloss
```sh
sudo vi /etc/systemd/system/photoprism_docker_compose_up.service
```
update accordingly and add
```
[Unit]
Description=BootPhotoprismAfterPowerLoss

[Service]
Type=simple
ExecStart=docker compose up -d
User=root
Restart=no
WorkingDirectory=/home/friendlyantz

[Install]
WantedBy=multi-user.target
EOF
```


## Networking / Firewall (WIP)

 **Options and Considerations**

1. Internet Router --> Managable Switch --> 
  - Server with PFSense AND  
  - Access Points for 
    - separate VLANs / PFSense tags --> Devices
    - Secure Net
    - Home Automation 
    - etc
2. Internet Router w/o switch
  - LAN to Proxmox with PFSense 
    - --> USB WiFi Dongle to use as Access Point for IoT devices
    - Normal internet / WiFi for Secure Net
3. Internet Router with VLANs
  - LAN to Proxmox with PFSense to tag VLANs
    - VLAN for IoT
    - VLAN for Secure Net
    - VLAN for Home Automation / etc

### PFsense in Proxmox

1. Proxmox ---> Ensure you create a virtual network bridge that you can use as LAN for PFSense, because as soon as you create another WLAN / network firewall will block all you from accessing original ETH/WAN ip address
1. pfSense/Interfaces: Create new LAN assigngment for your LAN bridge
1. pfSense/Interfaces: Create new OPT assigngment for your WLAN
1. DHCP Server: Enable DHCP server for WLAN

### Add USB dongle

> WIP

### Firewall

Remove all access unless required.
Top-down rule: start at Datacenter --> Cluster / Host --> VM
Rules are applied just to that level(i.e Datacenter, Cluster, VM), they are not inherited.

#### Add new rule (some examples below)

| setting / protocol | tcp(for web ui / Proxmox console[note this also enables SSH if you do not specify the port]) | icmp(for ping, etc, OPTIONAL) | tcp for SSH |
| --- | --- | --- |                                            --- |
| Macro | --- | --- |                                            SSH |
| Protocol | tcp | icmp |                                      tcp(not required if using Macro) |
| Interface| `vmbr0` | --- |                                   --- |
| Source port | 8006 | --- |                                    --- |
| Destination port | 8006 | --- |                              22(not required if using Macro) |
| Source | --- | `ip_of_your_machine`/32  |                    --- |

tick enabled

Note: TCP without specified port also enables SSH, so you can use that instead of Macro

#### Firewall Options

- [ ] Enable: `YES`
- [ ] Input / Output policies

---
## External Storage

```sh
fdisk -l # to find the drive

mkdir /mnt/usb_data

mount /dev/sdb1 /mnt/usb_data # mount the drive
```

```sh
lsblk # to find the drive
mkfs -t ext4 /dev/sda # format the drive OR do it via UI
# encryption
cryptsetup luksFormat /dev/sda # encrypt the drive
```
